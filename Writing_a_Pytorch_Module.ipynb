{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Writing a Pytorch Module",
      "provenance": [],
      "authorship_tag": "ABX9TyPNLwglOjRekdNGHEMJQv/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanraja753/PyTorch_Learning/blob/main/Writing_a_Pytorch_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FH4a9mBaIeHz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- We now have all the bricks needed to build our first convolutional network from scratch. The last technical point is the tensor shape between layers.\n",
        "\n",
        "- Both the convolutional and pooling layers take as input batches of samples, each one being itself a 3d tensor `C × H × W` .\n",
        "The output has the same structure, and tensors have to be explicitly reshaped before being forwarded to a fully connected layer."
      ],
      "metadata": {
        "id": "fdixQ_yUI0_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = MNIST('./data/mnist/', train = True, download = True)\n",
        "d = mnist.train_data\n",
        "d.size()"
      ],
      "metadata": {
        "id": "FgsG6Zy6Iob6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = d.view(d.size(0),1,d.size(1),d.size(2))\n",
        "x2 = d.unsqueeze(1)"
      ],
      "metadata": {
        "id": "Y7gy77iBJF8F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(x2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp7_gPHjJXn4",
        "outputId": "f74e0e4e-2c60-4b1f-87f4-6047ae3d0eee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 1, 28, 28])\n",
            "torch.Size([60000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.view(x.size(0),-1)\n",
        "x.size()"
      ],
      "metadata": {
        "id": "3jTCZPl0JbCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd77023-858c-4a51-dfbe-250c41597fa9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using `-1` for one of the `Tensor.view()’`s arguments automatically computes the proper value, given the original tensor size and the other dimensions."
      ],
      "metadata": {
        "id": "vg23JY1--9YX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PyTorch offers a sequential container moduele `torch.nn.Sequential` to build simple architectures.\n",
        "\n",
        "- For instance a MLP with a `10` dimension input, `2` dimension output, `ReLU` activation and two hidden layers of dimensions `100` and `50` can be written as:\n",
        "\n"
      ],
      "metadata": {
        "id": "fE8aTmIW_QUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(10,100), nn.ReLU(),\n",
        "    nn.Linear(100,50), nn.ReLU(),\n",
        "    nn.Linear(50,2)\n",
        ")"
      ],
      "metadata": {
        "id": "96G6iX90-4fz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However for any model of reasonable complexity, the best is to write a sub-class of `torch.nn.Module.`"
      ],
      "metadata": {
        "id": "lOrbqouQ_-IR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a `Module`, one has to inherit from the base class and implement the constructor `__init__(self, ...)` and the forward pass `forward(self, x).`"
      ],
      "metadata": {
        "id": "b7au2cMDAE0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(256, 200)\n",
        "        self.fc2 = nn.Linear(200, 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x =  F.relu(F.max_pool2d(self.conv1(x),kernel_size=3, stride=3))\n",
        "        x =  F.relu(F.max_pool2d(self.conv2(x),kernel_size=2, stride=2))\n",
        "        x =  x.view(-1,256)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zVRA7kzy_662"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inheriting from `torch.nn.Module` provides many mechanisms implemented in the superclass\n",
        "\n",
        "First, the `(...)` operator is redefined to call the `forward(...)` methods and run additional operations. The forward pass should be executed through this operator and not by calling `forward` explicitely\n",
        "\n",
        "Using the class`Net` we just defined"
      ],
      "metadata": {
        "id": "9Dcw77OJBiJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "input = torch.empty(12,1,28,28).normal_()\n",
        "output= model(input)\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUWdpoQNBePk",
        "outputId": "04bddbc5-6e34-42b8-ae27-238b1dd6c946"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, the `Parameters` added as class attributes, or from modules added as class attributes, are seen by `Module.parameters()`"
      ],
      "metadata": {
        "id": "ihj9RFAzCZtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, k in model.named_parameters():\n",
        "    print(n,k.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMhUR-7CCNJ_",
        "outputId": "29dca7b5-934d-4868-c28f-1ea357d8b191"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight torch.Size([32, 1, 5, 5])\n",
            "conv1.bias torch.Size([32])\n",
            "conv2.weight torch.Size([64, 32, 5, 5])\n",
            "conv2.bias torch.Size([64])\n",
            "fc1.weight torch.Size([200, 256])\n",
            "fc1.bias torch.Size([200])\n",
            "fc2.weight torch.Size([10, 200])\n",
            "fc2.bias torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parameters added in dictionaries or arrays are not seen.*\n"
      ],
      "metadata": {
        "id": "7hXjqDaMC4z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buggy(nn.Module): \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(1, 32, kernel_size=5) \n",
        "        self.param = nn.Parameter(torch.zeros(123, 456)) \n",
        "        self.other_stuff = [ nn.Linear(543, 21) ]\n",
        "model = Buggy()\n",
        "for n, k in model.named_parameters(): \n",
        "    print(n, k.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEECOzijCudy",
        "outputId": "43d24a95-b658-44f9-8756-db5ecfee0acc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "param torch.Size([123, 456])\n",
            "conv.weight torch.Size([32, 1, 5, 5])\n",
            "conv.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple option is to add modules in a `torch.nn.ModuleList,` which is a list of modules properly dealt with by PyTorch’s machinery."
      ],
      "metadata": {
        "id": "03lzklnrDVvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NotBuggy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(1,32,kernel_size = 5)\n",
        "        self.param = nn.Parameter(torch.zeros(123,456))\n",
        "        self.other_stuff = nn.ModuleList()\n",
        "        self.other_stuff.append(nn.Linear(543,21))\n",
        "\n",
        "model = NotBuggy()\n",
        "\n",
        "\n",
        "for n, k in model.named_parameters():\n",
        "    print(n,k.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpT2Wp0QDC9J",
        "outputId": "7e5e1462-0b88-4398-e198-9cad2020a9e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "param torch.Size([123, 456])\n",
            "conv.weight torch.Size([32, 1, 5, 5])\n",
            "conv.bias torch.Size([32])\n",
            "other_stuff.0.weight torch.Size([21, 543])\n",
            "other_stuff.0.bias torch.Size([21])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As long as you use autograd-compliant operations, the backward pass is implemented automatically.\n",
        "\n",
        "- This is crucial to allow the optimization of the Parameters with gradient descent."
      ],
      "metadata": {
        "id": "RJlk0MEzECxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S9xjPaSiD-Bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}