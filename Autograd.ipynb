{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO33cg/GXElCgLxekfHDGS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanraja753/PyTorch_Learning/blob/main/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Autograd"
      ],
      "metadata": {
        "id": "J3dxRshcXBTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conceptually, the forward pass is a standard tensor computation, and the DAG of tensor operations is required only to compute derivatives.\n",
        "\n",
        "When executing tensor operations, PyTorch can automatically construct on-the-fly the graph of operations to compute the gradient of any quantity with respect to any tensor involved.\n",
        "\n",
        "- Simpler syntax: one just needs to write the forward pass as a standard sequence of Python operations,\n",
        "\n",
        "- greater flexibility: since the graph is not static, the forward pass can be dynamically modulated."
      ],
      "metadata": {
        "id": "zumFlvDrXIQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `Tensor` has a Boolean field `requires_grad`, set to `False` by default, which states if `PyTorch` should build the graph of operations so that gradients with respect to it can be computed.\n",
        "\n",
        "\n",
        "The result of a tensorial operation has this flag to `True` if any of its operand has it to `True`."
      ],
      "metadata": {
        "id": "RP4L-heEXYwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "DA-rhr8OX32x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mQufumj4W7di"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.0, 2.0])\n",
        "y = torch.tensor([4.0, 5.0])\n",
        "z = torch.tensor([7.0, 3.0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQvYH9CjX1Nr",
        "outputId": "cf81b263-2358-429a-a5ee-7f5db7aaf67c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x+y).requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eejSkvoPX7q9",
        "outputId": "685ac6bc-7870-4ca9-b7c8-a3cb7344a89b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.requires_grad = True\n",
        "(x+z).requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Q5AejQX_jU",
        "outputId": "c67aa59e-7c66-4b6e-8dc4-d99bcae56915"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only floating point type tensors can have their gradient computed."
      ],
      "metadata": {
        "id": "MrqF0UquYIUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1., 10.],requires_grad=True)"
      ],
      "metadata": {
        "id": "n3vXt9l6YFBZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 10],requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "Stw2_1okYM98",
        "outputId": "2990631e-04ea-427e-82bc-0d0559cd6bd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-24d9391a891e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.autograd.grad(outputs, inputs)` computes and returns the gradient of outputs with respect to inputs."
      ],
      "metadata": {
        "id": "1zdvjkInYXwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([1., 2., 4.]).requires_grad_()\n",
        "u = torch.tensor([10., 20.]).requires_grad_()\n",
        "a = t.pow(2).sum() + u.log().sum()\n",
        "torch.autograd.grad(a,(t,u))"
      ],
      "metadata": {
        "id": "6skYIiYlYPXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`inputs` can be a single tensor, but the result is still a `[one element] tuple.`\n",
        "\n",
        "If `outputs` is a `tuple`, the result is the `sum of the gradients` of its elements."
      ],
      "metadata": {
        "id": "r4bqrENMaN0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "The function `Tensor.backward()` accumulates gradients in the `grad` fields of tensors which are not results of operations, the “leaves” in the autograd graph."
      ],
      "metadata": {
        "id": "qrmGY3hHarXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([ -3., 2., 5. ]).requires_grad_()\n",
        "u = x.pow(3).sum()\n",
        "print(x.grad)\n",
        "u.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "TfkbSByJaBzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is an alternative to `torch.autograd.grad(...)` and standard for training models.\n",
        "\n",
        "<br>\n",
        "\n",
        "- `Tensor.grad()` is useful in context of deep-learning where the main use is gradient descent, because we need to subtract the gradient of a tensor to the tensor itself.\n",
        "- To do so with `autograd.grad()`, we would have to associate every gradient to its tensor.\n"
      ],
      "metadata": {
        "id": "6rMAe85cbFW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Tensor.backward()` accumulates the gradients in the grad fields of tensors, so one may have to set them to `zero` before calling it.**\n",
        "\n",
        "\n",
        "This accumulating behavior is desirable in particular to compute the gradient of a loss summed over several `mini-batches,` or the gradient of a sum of losses."
      ],
      "metadata": {
        "id": "MXv45HA4bVyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So we can run a forward/backward pass on"
      ],
      "metadata": {
        "id": "Vd7eaL8pboRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = torch.rand(5, 5).requires_grad_()\n",
        "w2 = torch.rand(5, 5).requires_grad_()\n",
        "x = torch.empty(5).normal_()"
      ],
      "metadata": {
        "id": "qiJ_9UUza9yw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = x\n",
        "x1 = w1 @ x\n",
        "x2 = x0 + w2 @ x1\n",
        "x3 = w1 @ (x1 + x2)\n",
        "\n",
        "q = x3.norm()\n",
        "\n",
        "q.backward()"
      ],
      "metadata": {
        "id": "IFXYtfHWbvbY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The difference between Tensorflow (as we saw in lecture 4.1. “DAG networks”) and PyTorch here is that variable q actually contains the result of the computation.\n",
        "\n",
        "- During the tensor operations, PyTorch built all the necessary operations to compute the gradient if needed.\n",
        "\n",
        "- When calling `q.backward()`, `PyTorch` actually runs this built graph to fill the grad fields of the parameters."
      ],
      "metadata": {
        "id": "uhyQfgVpcQOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Autograd Machinery"
      ],
      "metadata": {
        "id": "Az1LB-zYcbPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the full graph built during a computation."
      ],
      "metadata": {
        "id": "NP7UAxJ5qdHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `torch.no_grad()` context switches off the autograd machinery, and can be used for operations such as parameter updates\n"
      ],
      "metadata": {
        "id": "oHjpkKG9sA4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.empty(10, 784).normal_(0, 1e-3).requires_grad_()\n",
        "b = torch.empty(10).normal_(0, 1e-3).requires_grad_()"
      ],
      "metadata": {
        "id": "cwoXTDB7cBa3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7eHMVUrFtcXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1000,784)\n",
        "y = torch.randn(1000,1)\n",
        "for  k in range(100):\n",
        "    y_hat = x @ w.t() + b\n",
        "    loss = (y_hat - y).pow(2).mean()\n",
        "    w.grad, b.grad = None, None\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        w -= 0.1*w.grad\n",
        "        b -= 0.1*b.grad\n",
        "\n",
        "    print(w)"
      ],
      "metadata": {
        "id": "3XNEOjAGsdnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The `detach()` method creates a tensor which shares the data, but does not require gradient computation, and is not connected to the current graph.\n",
        "- This method should be used when the gradient should not be propagated beyond a variable, or to update leaf tensors."
      ],
      "metadata": {
        "id": "dbR1-j_otn1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(0.5,requires_grad=True)\n",
        "b = torch.tensor(-0.5, requires_grad=True)\n",
        "\n",
        "for k in range(100):\n",
        "    loss = (a - 1)**2 + (b + 1)**2 + (a - b)**2\n",
        "    torch.autograd.grad(loss, (a,b))\n",
        "    with torch.no_grad():\n",
        "        a = a - grad_a\n",
        "        b = b - grad_b\n",
        "\n",
        "#print(a,b)"
      ],
      "metadata": {
        "id": "1Yy0NL8StVHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ajCR_n7yuhfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}